{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353, 10) (353,)\n",
      "(89, 10) (89,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "x, y = load_diabetes(return_X_y=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "print(x_train_scaled.shape, y_train.shape)\n",
    "print(x_test_scaled.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([353, 10]) torch.Size([353, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "## regression: loss function and metric\n",
    "mse_loss = nn.MSELoss()     \n",
    "\n",
    "def r2_score(y_pred, y_true):\n",
    "    mean_y = torch.mean(y_true)\n",
    "    ss_tot = torch.sum((y_true - mean_y)**2)\n",
    "    ss_res = torch.sum((y_true - y_pred)**2)  \n",
    "    return 1 - (ss_res / ss_tot) \n",
    "\n",
    "# Hyperparameters\n",
    "n_epochs = 50000\n",
    "learning_rate = 0.1\n",
    "\n",
    "input_dim = 10\n",
    "hidden_dim = 100\n",
    "output_dim = 1\n",
    "\n",
    "# Data\n",
    "x = torch.tensor(x_train_scaled).float()\n",
    "y = torch.tensor(y_train).float().view(-1, 1)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Method-1] Manual Backpropagation + Manual Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000/50000] loss: 1779.0012 score: 0.7072\n",
      "[10000/50000] loss: 1635.9414 score: 0.7308\n",
      "[15000/50000] loss: 1564.4283 score: 0.7425\n",
      "[20000/50000] loss: 1514.8424 score: 0.7507\n",
      "[25000/50000] loss: 1503.7875 score: 0.7525\n",
      "[30000/50000] loss: 1497.9303 score: 0.7535\n",
      "[35000/50000] loss: 1496.4808 score: 0.7537\n",
      "[40000/50000] loss: 1496.4729 score: 0.7537\n",
      "[45000/50000] loss: 1491.8660 score: 0.7545\n",
      "[50000/50000] loss: 1489.6545 score: 0.7548\n"
     ]
    }
   ],
   "source": [
    "## Method - 1\n",
    "## Model\n",
    "torch.manual_seed(42)\n",
    "w1 = torch.randn(input_dim, hidden_dim).requires_grad_(False)\n",
    "b1 = torch.zeros(hidden_dim).requires_grad_(False)\n",
    "w2 = torch.randn(hidden_dim, output_dim).requires_grad_(False)\n",
    "b2 = torch.zeros(output_dim).requires_grad_(False)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Forward propagation\n",
    "    z1 = torch.mm(x, w1) + b1\n",
    "    a1 = torch.sigmoid(z1)\n",
    "    z2 = torch.mm(a1, w2) + b2\n",
    "    a2 = z2     ## Identity activation\n",
    "\n",
    "    y_pred = a2\n",
    "    loss = mse_loss(y_pred, y)\n",
    "    score = r2_score(y_pred, y)\n",
    "\n",
    "    # Backward progapation\n",
    "    grad_a2 = 2 * (a2 - y) / y.shape[0]\n",
    "    grad_z2 = grad_a2\n",
    "    grad_w2 = torch.mm(a1.t(), grad_z2)\n",
    "    grad_b2 = torch.sum(grad_z2, dim=0)\n",
    "    \n",
    "    grad_a1 = torch.mm(grad_z2, w2.t())\n",
    "    grad_z1 = a1 * (1 - a1) * grad_a1\n",
    "    grad_w1 = torch.mm(x.t(), grad_z1)\n",
    "    grad_b1 = torch.sum(grad_z1, dim=0)\n",
    "\n",
    "    # Update weights and biases\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    b1 -= learning_rate * grad_b1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    b2 -= learning_rate * grad_b2\n",
    "\n",
    "    if epoch % (n_epochs // 10) == 0:\n",
    "        print(f\"[{epoch}/{n_epochs}] loss: {loss.item():.4f} score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Method-2] torch.autograd.grad() + Manual Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000/50000] loss: 1842.9210 score: 0.6967\n",
      "[10000/50000] loss: 1635.1206 score: 0.7309\n",
      "[15000/50000] loss: 1584.1705 score: 0.7393\n",
      "[20000/50000] loss: 1552.9069 score: 0.7444\n",
      "[25000/50000] loss: 1542.6190 score: 0.7461\n",
      "[30000/50000] loss: 1494.4867 score: 0.7541\n",
      "[35000/50000] loss: 1491.6666 score: 0.7545\n",
      "[40000/50000] loss: 1486.2555 score: 0.7554\n",
      "[45000/50000] loss: 1463.4923 score: 0.7592\n",
      "[50000/50000] loss: 1445.4012 score: 0.7621\n"
     ]
    }
   ],
   "source": [
    "## Method - 2\n",
    "## Model\n",
    "torch.manual_seed(42)\n",
    "w1 = torch.randn(input_dim, hidden_dim).requires_grad_(True)\n",
    "b1 = torch.zeros(hidden_dim).requires_grad_(True)\n",
    "w2 = torch.randn(hidden_dim, output_dim).requires_grad_(True)\n",
    "b2 = torch.zeros(output_dim).requires_grad_(True)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Forward propagation\n",
    "    z1 = torch.mm(x, w1) + b1\n",
    "    a1 = torch.sigmoid(z1)\n",
    "    z2 = torch.mm(a1, w2) + b2\n",
    "    a2 = z2     ## Identity activation\n",
    "\n",
    "    y_pred = a2\n",
    "    loss = mse_loss(y_pred, y)\n",
    "    score = r2_score(y_pred, y)\n",
    "\n",
    "    # Backward progapation\n",
    "    grads = torch.autograd.grad(loss, [w1, b1, w2, b2], create_graph=True)\n",
    "\n",
    "    # Update weights and biases\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * grads[0]\n",
    "        b1 -= learning_rate * grads[1]\n",
    "        w2 -= learning_rate * grads[2]\n",
    "        b2 -= learning_rate * grads[3]\n",
    "        \n",
    "    if epoch % (n_epochs // 10) == 0:\n",
    "        print(f\"[{epoch}/{n_epochs}] loss: {loss.item():.4f} score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Method-3] loss.backward() + Manual Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000/50000] loss: 1842.9210 score: 0.6967\n",
      "[10000/50000] loss: 1635.1206 score: 0.7309\n",
      "[15000/50000] loss: 1584.1705 score: 0.7393\n",
      "[20000/50000] loss: 1552.9069 score: 0.7444\n",
      "[25000/50000] loss: 1542.6190 score: 0.7461\n",
      "[30000/50000] loss: 1494.4867 score: 0.7541\n",
      "[35000/50000] loss: 1491.6666 score: 0.7545\n",
      "[40000/50000] loss: 1486.2555 score: 0.7554\n",
      "[45000/50000] loss: 1463.4923 score: 0.7592\n",
      "[50000/50000] loss: 1445.4012 score: 0.7621\n"
     ]
    }
   ],
   "source": [
    "## Model - 3\n",
    "torch.manual_seed(42)\n",
    "w1 = torch.randn(input_dim, hidden_dim).requires_grad_(True)\n",
    "b1 = torch.zeros(hidden_dim).requires_grad_(True)\n",
    "w2 = torch.randn(hidden_dim, output_dim).requires_grad_(True)\n",
    "b2 = torch.zeros(output_dim).requires_grad_(True)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Forward propagation\n",
    "    z1 = torch.mm(x, w1) + b1\n",
    "    a1 = torch.sigmoid(z1)\n",
    "    z2 = torch.mm(a1, w2) + b2\n",
    "    a2 = z2     ## Identity activation\n",
    "\n",
    "    y_pred = a2\n",
    "    loss = mse_loss(y_pred, y)\n",
    "    score = r2_score(y_pred, y)\n",
    "\n",
    "    # Backward progapation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights and biases\n",
    "    with torch.no_grad():    \n",
    "        w1 -= learning_rate * w1.grad\n",
    "        b1 -= learning_rate * b1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        b2 -= learning_rate * b2.grad\n",
    "\n",
    "        w1.grad.zero_()\n",
    "        b1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "        b2.grad.zero_()\n",
    "\n",
    "    if epoch % (n_epochs // 10) == 0:\n",
    "        print(f\"[{epoch}/{n_epochs}] loss: {loss.item():.4f} score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Method-4] loss.backward() + Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000/50000] loss: 1803.7753 score: 0.7032\n",
      "[10000/50000] loss: 1682.4138 score: 0.7231\n",
      "[15000/50000] loss: 1582.1388 score: 0.7396\n",
      "[20000/50000] loss: 1579.2837 score: 0.7401\n",
      "[25000/50000] loss: 1562.5255 score: 0.7429\n",
      "[30000/50000] loss: 1557.1533 score: 0.7437\n",
      "[35000/50000] loss: 1556.9713 score: 0.7438\n",
      "[40000/50000] loss: 1556.1246 score: 0.7439\n",
      "[45000/50000] loss: 1556.5000 score: 0.7438\n",
      "[50000/50000] loss: 1529.8906 score: 0.7482\n"
     ]
    }
   ],
   "source": [
    "## Method - 4\n",
    "## Model\n",
    "torch.manual_seed(42)\n",
    "w1 = torch.randn(input_dim, hidden_dim).requires_grad_(True)\n",
    "b1 = torch.zeros(hidden_dim).requires_grad_(True)\n",
    "w2 = torch.randn(hidden_dim, output_dim).requires_grad_(True)\n",
    "b2 = torch.zeros(output_dim).requires_grad_(True)\n",
    "\n",
    "optimizer = optim.SGD([w1, b1, w2, b2], lr=0.1)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Forward propagation\n",
    "    lin1 = torch.mm(x, w1) + b1\n",
    "    act1 = torch.sigmoid(lin1)\n",
    "    out = torch.mm(act1, w2) + b2\n",
    "    y_pred = out\n",
    "\n",
    "    loss = mse_loss(y_pred, y)\n",
    "    score = r2_score(y_pred, y)\n",
    "\n",
    "    # Backward progapation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % (n_epochs // 10) == 0:\n",
    "        print(f\"[{epoch}/{n_epochs}] loss: {loss.item():.4f} score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method-5] nn.Module + loss.backward() + Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000/50000] loss: 2177.4368 score: 0.6417\n",
      "[10000/50000] loss: 2098.9351 score: 0.6546\n",
      "[15000/50000] loss: 1947.5015 score: 0.6795\n",
      "[20000/50000] loss: 1906.4879 score: 0.6862\n",
      "[25000/50000] loss: 1844.3549 score: 0.6965\n",
      "[30000/50000] loss: 1824.8132 score: 0.6997\n",
      "[35000/50000] loss: 1805.3464 score: 0.7029\n",
      "[40000/50000] loss: 1804.2379 score: 0.7031\n",
      "[45000/50000] loss: 1786.3413 score: 0.7060\n",
      "[50000/50000] loss: 1781.1143 score: 0.7069\n"
     ]
    }
   ],
   "source": [
    "## Method - 5\n",
    "## Model\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(input_dim, hidden_dim),\n",
    "#     nn.Sigmoid(),\n",
    "#     nn.Linear(hidden_dim, output_dim),\n",
    "# )\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        ## initialization\n",
    "        torch.nn.init.normal_(self.linear1.weight)\n",
    "        torch.nn.init.normal_(self.linear2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.linear2(x)  \n",
    "        return x\n",
    "\n",
    "model = MLP(input_dim, hidden_dim, output_dim)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Forward propagation\n",
    "    y_pred = model(x)\n",
    "    loss = mse_loss(y_pred, y)\n",
    "    score = r2_score(y_pred, y)\n",
    "\n",
    "    # Backward progapation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % (n_epochs // 10) == 0:\n",
    "        print(f\"[{epoch}/{n_epochs}] loss: {loss.item():.4f} score: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
